{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "african-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import SSD300, ResNet, Loss\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gothic-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/gulvarol/grocerydataset/master/annotations.csv\",\n",
    "                 names=[\"names\", \"x\", \"y\", \"w\", \"h\", \"class\"])\n",
    "\n",
    "df['class'].replace(df[\"class\"].unique(), '1')\n",
    "\n",
    "path = \"./dataset/ShelfImages/\"\n",
    "tr_path = os.path.join(path, \"train/\")\n",
    "ts_path = os.path.join(path, \"test/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sharp-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class DataEncoder:\n",
    "    def __init__(self):\n",
    "        '''Compute default box sizes with scale and aspect transform.'''\n",
    "        scale = 300.\n",
    "        steps = [s / scale for s in (8, 16, 32, 64, 100, 300)]\n",
    "        sizes = [s / scale for s in (30, 60, 111, 162, 213, 264, 315)]\n",
    "        aspect_ratios = ((2,), (2,3), (2,3), (2,3), (2,), (2,))\n",
    "        feature_map_sizes = (38, 19, 10, 5, 3, 1)\n",
    "\n",
    "        num_layers = len(feature_map_sizes)\n",
    "\n",
    "        boxes = []\n",
    "\n",
    "\n",
    "        mean = []\n",
    "        x = 0\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            fmsize = feature_map_sizes[i]\n",
    "\n",
    "            for h,w in itertools.product(range(fmsize), repeat=2):\n",
    "                cx = (w+0.5)*steps[i]\n",
    "                cy = (h+0.5)*steps[i]\n",
    "                bbox = [cx,cy, 0.109, 0.12]\n",
    "                boxes.append(bbox)\n",
    "        \n",
    "        self.default_boxes = torch.Tensor(boxes)\n",
    "\n",
    "\n",
    "\n",
    "    def iou(self, box1, box2):\n",
    "        '''Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n",
    "\n",
    "        Args:\n",
    "          box1: (tensor) bounding boxes, sized [N,4].\n",
    "          box2: (tensor) bounding boxes, sized [M,4].\n",
    "\n",
    "        Return:\n",
    "          (tensor) iou, sized [N,M].\n",
    "        '''\n",
    "        N = box1.size(0)\n",
    "        M = box2.size(0)\n",
    "\n",
    "        lt = torch.max(\n",
    "            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "        )\n",
    "\n",
    "        rb = torch.min(\n",
    "            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "        )\n",
    "\n",
    "        wh = rb - lt  # [N,M,2]\n",
    "        wh[wh<0] = 0  # clip at 0\n",
    "        inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n",
    "\n",
    "        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\n",
    "        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\n",
    "        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n",
    "        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n",
    "\n",
    "        iou = inter / (area1 + area2 - inter)\n",
    "        return iou\n",
    "\n",
    "    def encode(self, boxes, classes, threshold=0.5):\n",
    "        '''Transform target bounding boxes and class labels to SSD boxes and classes.\n",
    "\n",
    "        Match each object box to all the default boxes, pick the ones with the\n",
    "        Jaccard-Index > 0.5:\n",
    "            Jaccard(A,B) = AB / (A+B-AB)\n",
    "\n",
    "        Args:\n",
    "          boxes: (tensor) object bounding boxes (xmin,ymin,xmax,ymax) of a image, sized [#obj, 4].\n",
    "          classes: (tensor) object class labels of a image, sized [#obj,].\n",
    "          threshold: (float) Jaccard index threshold\n",
    "\n",
    "        Returns:\n",
    "          boxes: (tensor) bounding boxes, sized [#obj, 8732, 4].\n",
    "          classes: (tensor) class labels, sized [8732,]\n",
    "        '''\n",
    "#         print(f\"Default box shape: {self.default_boxes.shape}\")\n",
    "        default_boxes = self.default_boxes\n",
    "        num_default_boxes = default_boxes.size(0)\n",
    "        num_objs = boxes.size(0)\n",
    "        \n",
    "        \n",
    "        iou = self.iou(  # [#obj,8732]\n",
    "            boxes,\n",
    "            torch.cat([default_boxes[:,:2] - default_boxes[:,2:]/2,\n",
    "                       default_boxes[:,:2] + default_boxes[:,2:]/2], 1)\n",
    "        )\n",
    "\n",
    "#         print(f'iou shape: {iou.shape}')\n",
    "\n",
    "        \n",
    "        iou, max_idx = iou.max(0)  # [1,8732]\n",
    "\n",
    "        max_idx.squeeze_(0)        # [8732,]\n",
    "        iou.squeeze_(0)            # [8732,]\n",
    "        \n",
    "#         print(f'iou sq shape: {iou.shape}')\n",
    "\n",
    "        \n",
    "        boxes = boxes[max_idx]     # [8732,4]\n",
    "        \n",
    "\n",
    "#         print(f'def box shape: {boxes.shape}')\n",
    "         \n",
    "        variances = [0.1, 0.2]\n",
    "        cxcy = (boxes[:,:2] + boxes[:,2:])/2 - default_boxes[:,:2]  # [8732,2]\n",
    "        cxcy /= variances[0] * default_boxes[:,2:]\n",
    "        wh = (boxes[:,2:] - boxes[:,:2]) / default_boxes[:,2:]      # [8732,2]\n",
    "        wh = torch.log(wh) / variances[1]\n",
    "        loc = torch.cat([cxcy, wh], 1)  # [8732,4]\n",
    "\n",
    "        conf = 1 + classes[max_idx]   # [8732,], background class = 0\n",
    "        conf[iou<threshold] = 0       # background\n",
    "        return loc, conf\n",
    "\n",
    "    def nms(self, bboxes, scores, threshold=0.5, mode='union'):\n",
    "        '''Non maximum suppression.\n",
    "\n",
    "        Args:\n",
    "          bboxes: (tensor) bounding boxes, sized [N,4].\n",
    "          scores: (tensor) bbox scores, sized [N,].\n",
    "          threshold: (float) overlap threshold.\n",
    "          mode: (str) 'union' or 'min'.\n",
    "\n",
    "        Returns:\n",
    "          keep: (tensor) selected indices.\n",
    "\n",
    "        Ref:\n",
    "          https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/nms/py_cpu_nms.py\n",
    "        '''\n",
    "        x1 = bboxes[:,0]\n",
    "        y1 = bboxes[:,1]\n",
    "        x2 = bboxes[:,2]\n",
    "        y2 = bboxes[:,3]\n",
    "\n",
    "        areas = (x2-x1) * (y2-y1)\n",
    "        _, order = scores.sort(0, descending=True)\n",
    "\n",
    "        keep = []\n",
    "        while order.numel() > 0:\n",
    "            i = order[0]\n",
    "            keep.append(i)\n",
    "\n",
    "            if order.numel() == 1:\n",
    "                break\n",
    "\n",
    "            xx1 = x1[order[1:]].clamp(min=x1[i])\n",
    "            yy1 = y1[order[1:]].clamp(min=y1[i])\n",
    "            xx2 = x2[order[1:]].clamp(max=x2[i])\n",
    "            yy2 = y2[order[1:]].clamp(max=y2[i])\n",
    "\n",
    "            w = (xx2-xx1).clamp(min=0)\n",
    "            h = (yy2-yy1).clamp(min=0)\n",
    "            inter = w*h\n",
    "\n",
    "            if mode == 'union':\n",
    "                ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "            elif mode == 'min':\n",
    "                ovr = inter / areas[order[1:]].clamp(max=areas[i])\n",
    "            else:\n",
    "                raise TypeError('Unknown nms mode: %s.' % mode)\n",
    "\n",
    "            ids = (ovr<=threshold).nonzero().squeeze()\n",
    "            if ids.numel() == 0:\n",
    "                break\n",
    "            order = order[ids+1]\n",
    "        return torch.LongTensor(keep)\n",
    "\n",
    "    def decode(self, loc, conf):\n",
    "        '''Transform predicted loc/conf back to real bbox locations and class labels.\n",
    "\n",
    "        Args:\n",
    "          loc: (tensor) predicted loc, sized [8732,4].\n",
    "          conf: (tensor) predicted conf, sized [8732,21].\n",
    "\n",
    "        Returns:\n",
    "          boxes: (tensor) bbox locations, sized [#obj, 4].\n",
    "          labels: (tensor) class labels, sized [#obj,1].\n",
    "        '''\n",
    "        variances = [0.1, 0.2]\n",
    "        wh = torch.exp(loc[:,2:]*variances[1]) * self.default_boxes[:,2:]\n",
    "        cxcy = loc[:,:2] * variances[0] * self.default_boxes[:,2:] + self.default_boxes[:,:2]\n",
    "        boxes = torch.cat([cxcy-wh/2, cxcy+wh/2], 1)  # [8732,4]\n",
    "\n",
    "        max_conf, labels = conf.max(1)  # [8732,1]\n",
    "        ids = labels.squeeze(1).nonzero().squeeze(1)  # [#boxes,]\n",
    "\n",
    "        keep = self.nms(boxes[ids], max_conf[ids].squeeze(1))\n",
    "        return boxes[ids][keep], labels[ids][keep], max_conf[ids][keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "peaceful-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(data.Dataset):\n",
    "    img_size = 300\n",
    "\n",
    "    def __init__(self, root=None, df=None, train=None, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "          root: (str) ditectory to images.\n",
    "          list_file: (str) path to index file.\n",
    "          train: (boolean) train or test.\n",
    "          transform: ([transforms]) image transforms.\n",
    "        '''\n",
    "        self.root = root\n",
    "        \n",
    "        \n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        self.fnames = []\n",
    "        self.boxes = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.data_encoder = DataEncoder()\n",
    "        self.df = df\n",
    "        self.bbox_cols = [\"x\", \"y\",\"w\",\"h\"]\n",
    "        self.image_list = os.listdir(root) \n",
    "        self.num_samples = len(self.image_list)\n",
    "        \n",
    "\n",
    "        for img in self.image_list:\n",
    "            \n",
    "            self.fnames.append(img)\n",
    "            split_df = df[df[\"names\"]==img]\n",
    "            cls_ls, bbox = split_df[\"class\"].values, split_df[self.bbox_cols].values      \n",
    "            self.boxes.append(torch.Tensor(bbox))\n",
    "            self.labels.append(torch.LongTensor(cls_ls))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Load a image, and encode its bbox locations and class labels.\n",
    "\n",
    "        Args:\n",
    "          idx: (int) image index.\n",
    "\n",
    "        Returns:\n",
    "          img: (tensor) image tensor.\n",
    "          loc_target: (tensor) location targets, sized [8732,4].\n",
    "          conf_target: (tensor) label targets, sized [8732,].\n",
    "        '''\n",
    "        # Load image and bbox locations.\n",
    "        fname = self.fnames[idx]\n",
    "        img = Image.open(os.path.join(self.root, fname))\n",
    "        boxes = self.boxes[idx].clone()\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # Data augmentation while training.\n",
    "        if self.train:\n",
    "            img, boxes = self.random_flip(img, boxes)\n",
    "            img, boxes, labels = self.random_crop(img, boxes, labels)\n",
    "\n",
    "        # Scale bbox locaitons to [0,1].\n",
    "        w,h = img.size\n",
    "        boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes)\n",
    "\n",
    "        img = img.resize((self.img_size,self.img_size))\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        \n",
    "        # Encode loc & conf targets.\n",
    "        loc_target, conf_target = self.data_encoder.encode(boxes, labels)\n",
    "#         print(f'box shape: {boxes.shape} lable shape: {labels.shape}')\n",
    "\n",
    "        return img, loc_target, conf_target\n",
    "\n",
    "    def random_flip(self, img, boxes):\n",
    "        '''Randomly flip the image and adjust the bbox locations.\n",
    "\n",
    "        For bbox (xmin, ymin, xmax, ymax), the flipped bbox is:\n",
    "        (w-xmax, ymin, w-xmin, ymax).\n",
    "\n",
    "        Args:\n",
    "          img: (PIL.Image) image.\n",
    "          boxes: (tensor) bbox locations, sized [#obj, 4].\n",
    "\n",
    "        Returns:\n",
    "          img: (PIL.Image) randomly flipped image.\n",
    "          boxes: (tensor) randomly flipped bbox locations, sized [#obj, 4].\n",
    "        '''\n",
    "        if random.random() < 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w = img.width\n",
    "            xmin = w - boxes[:,2]\n",
    "            xmax = w - boxes[:,0]\n",
    "            boxes[:,0] = xmin\n",
    "            boxes[:,2] = xmax\n",
    "        return img, boxes\n",
    "\n",
    "    def random_crop(self, img, boxes, labels):\n",
    "        '''Randomly crop the image and adjust the bbox locations.\n",
    "\n",
    "        For more details, see 'Chapter2.2: Data augmentation' of the paper.\n",
    "\n",
    "        Args:\n",
    "          img: (PIL.Image) image.\n",
    "          boxes: (tensor) bbox locations, sized [#obj, 4].\n",
    "          labels: (tensor) bbox labels, sized [#obj,].\n",
    "\n",
    "        Returns:\n",
    "          img: (PIL.Image) cropped image.\n",
    "          selected_boxes: (tensor) selected bbox locations.\n",
    "          labels: (tensor) selected bbox labels.\n",
    "        '''\n",
    "        imw, imh = img.size\n",
    "        while True:\n",
    "            min_iou = random.choice([None, 0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "            if min_iou is None:\n",
    "                return img, boxes, labels\n",
    "\n",
    "            for _ in range(100):\n",
    "                w = random.randrange(int(0.1*imw), imw)\n",
    "                h = random.randrange(int(0.1*imh), imh)\n",
    "\n",
    "                if h > 2*w or w > 2*h:\n",
    "                    continue\n",
    "\n",
    "                x = random.randrange(imw - w)\n",
    "                y = random.randrange(imh - h)\n",
    "                roi = torch.Tensor([[x, y, x+w, y+h]])\n",
    "\n",
    "                center = (boxes[:,:2] + boxes[:,2:]) / 2  # [N,2]\n",
    "                roi2 = roi.expand(len(center), 4)  # [N,4]\n",
    "                mask = (center > roi2[:,:2]) & (center < roi2[:,2:])  # [N,2]\n",
    "                mask = mask[:,0] & mask[:,1]  #[N,]\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                selected_boxes = boxes.index_select(0, mask.nonzero().squeeze(1))\n",
    "\n",
    "                iou = self.data_encoder.iou(selected_boxes, roi)\n",
    "                if iou.min() < min_iou:\n",
    "                    continue\n",
    "\n",
    "                img = img.crop((x, y, x+w, y+h))\n",
    "                selected_boxes[:,0].add_(-x).clamp_(min=0, max=w)\n",
    "                selected_boxes[:,1].add_(-y).clamp_(min=0, max=h)\n",
    "                selected_boxes[:,2].add_(-x).clamp_(min=0, max=w)\n",
    "                selected_boxes[:,3].add_(-y).clamp_(min=0, max=h)\n",
    "                return img, selected_boxes, labels[mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beautiful-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "from ssd import SSD300\n",
    "# from multibox_loss import MultiBoxLoss\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "best_loss = float('inf')  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "trainset = ListDataset(root=\"./dataset/ShelfImages/train/\", df=df, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = ListDataset(root='./dataset/ShelfImages/test/', df = df, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wired-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    num_classes = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.handle_nan_loss = None\n",
    "\n",
    "    def cross_entropy_loss(self, x, y):\n",
    "        '''Cross entropy loss w/o averaging across all samples.\n",
    "\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "\n",
    "        Return:\n",
    "          (tensor) cross entroy loss, sized [N,].\n",
    "        '''\n",
    "        xmax = x.data.max()\n",
    "        log_sum_exp = torch.log(torch.sum(torch.exp(x-xmax), 1)) + xmax\n",
    "        return log_sum_exp - x.gather(1, y.view(-1,1))\n",
    "\n",
    "    def test_cross_entropy_loss(self):\n",
    "        a = Variable(torch.randn(10,4))\n",
    "        b = Variable(torch.ones(10).long())\n",
    "        loss = self.cross_entropy_loss(a,b)\n",
    "#         print(loss.mean())\n",
    "#         print(F.cross_entropy(a,b))\n",
    "\n",
    "    def hard_negative_mining(self, conf_loss, pos):\n",
    "        '''Return negative indices that is 3x the number as postive indices.\n",
    "\n",
    "        Args:\n",
    "          conf_loss: (tensor) cross entroy loss between conf_preds and conf_targets, sized [N*1940,].\n",
    "          pos: (tensor) positive(matched) box indices, sized [N,1940].\n",
    "\n",
    "        Return:\n",
    "          (tensor) negative indices, sized [N,1940].\n",
    "        '''\n",
    "        batch_size, num_boxes = pos.size()\n",
    "\n",
    "        conf_loss[pos] = 0  # set pos boxes = 0, the rest are neg conf_loss\n",
    "        conf_loss = conf_loss.view(batch_size, -1)  # [N,1940]\n",
    "\n",
    "        _,idx = conf_loss.sort(1, descending=True)  # sort by neg conf_loss\n",
    "        _,rank = idx.sort(1)  # [N,1940]\n",
    "\n",
    "        num_pos = pos.long().sum(1)  # [N,1]\n",
    "        num_neg = torch.clamp(3*num_pos, max=num_boxes-1)  # [N,1]\n",
    "\n",
    "        neg = rank < num_neg.expand_as(rank)  # [N,1940]\n",
    "        return neg\n",
    "\n",
    "    def forward(self, loc_preds, loc_targets, conf_preds, conf_targets):\n",
    "        '''Compute loss between (loc_preds, loc_targets) and (conf_preds, conf_targets).\n",
    "\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [batch_size, 1940, 4].\n",
    "          loc_targets: (tensor) encoded target locations, sized [batch_size, 1940, 4].\n",
    "          conf_preds: (tensor) predicted class confidences, sized [batch_size, 1940, num_classes].\n",
    "          conf_targets: (tensor) encoded target classes, sized [batch_size, 1940].\n",
    "\n",
    "        loss:\n",
    "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + CrossEntropyLoss(conf_preds, conf_targets).\n",
    "        '''\n",
    "        flag = 1\n",
    "        batch_size, num_boxes, _ = loc_preds.size()\n",
    "\n",
    "        pos = conf_targets>0  # [N,1940], pos means the box matched.\n",
    "        num_matched_boxes = pos.data.long().sum()\n",
    "#         print(f'num of matched boxes: {num_matched_boxes}')\n",
    "\n",
    "        if num_matched_boxes == 0:\n",
    "            return self.handle_nan_loss\n",
    "#         else:\n",
    "\n",
    "        ################################################################\n",
    "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
    "        ################################################################\n",
    "        pos_mask = pos.unsqueeze(2).expand_as(loc_preds)    # [N,1940,4]\n",
    "        pos_loc_preds = loc_preds[pos_mask].view(-1,4)      # [#pos,4]\n",
    "        pos_loc_targets = loc_targets[pos_mask].view(-1,4)  # [#pos,4]\n",
    "        loc_loss = F.smooth_l1_loss(pos_loc_preds, pos_loc_targets, size_average=False)\n",
    "        loc_loss/= num_matched_boxes\n",
    "        flag = 1\n",
    "#         print(f'Location loss: {loc_loss}')\n",
    "        self.handle_nan_loss = loc_loss\n",
    "#        \n",
    "        return loc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "oriental-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SSD300()\n",
    "\n",
    "lr = 1e-3\n",
    "use_cuda = False\n",
    "\n",
    "criterion = MultiBoxLoss()\n",
    "\n",
    "# if use_cuda:\n",
    "#     net = torch.nn.DataParallel(net, device_ids=[0,1,2,3,4,5,6,7])\n",
    "#     net.cuda()\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), \n",
    "                      lr=lr,\n",
    "                      momentum=0.9,\n",
    "                      weight_decay=1e-4\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adverse-science",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 38, 38])\n",
      "9.823 9.823\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.906 9.864\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.955 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.916 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.527 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.560 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.887 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "6.556 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.405 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.889 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.047 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "8.675 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "7.908 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.224 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "3.124 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.460 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.228 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.279 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "8.463 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "8.714 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.639 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "nan nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.500 nan\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.005 nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5b404582b61c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloc_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cq/pytorch-ssd/ssd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# conv4_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "train_loss = 0\n",
    "for batch_idx, (images, loc_targets, conf_targets) in enumerate(trainloader):\n",
    "    if use_cuda:\n",
    "        images = images.cuda()\n",
    "        loc_targets = loc_targets.cuda()\n",
    "        conf_targets = conf_targets.cuda()\n",
    "        \n",
    "    \n",
    "    images = Variable(images)\n",
    "    \n",
    "    loc_targets = Variable(loc_targets)\n",
    "    \n",
    "    conf_targets = Variable(conf_targets)\n",
    "        \n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loc_preds, conf_preds = net(images)\n",
    "    \n",
    "#     try:\n",
    "\n",
    "    loss = Variable(criterion(loc_preds, loc_targets, \n",
    "                                  conf_preds, conf_targets),requires_grad =True)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.data\n",
    "    print('%.3f %.3f' % (loss.data, train_loss/(batch_idx+1)))\n",
    "\n",
    "#     except:\n",
    "#         print('Fix the loss function')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fancy-bubble",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b22ef7a211c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nan' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-western",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-advertiser",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-viking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
