{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "african-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import SSD300, ResNet, Loss\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gothic-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/gulvarol/grocerydataset/master/annotations.csv\",\n",
    "                 names=[\"names\", \"x\", \"y\", \"w\", \"h\", \"class\"])\n",
    "\n",
    "df['class'].replace(df[\"class\"].unique(), '1')\n",
    "\n",
    "path = \"./dataset/ShelfImages/\"\n",
    "tr_path = os.path.join(path, \"train/\")\n",
    "ts_path = os.path.join(path, \"test/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sharp-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class DataEncoder:\n",
    "    def __init__(self):\n",
    "        '''Compute default box sizes with scale and aspect transform.'''\n",
    "        scale = 300.\n",
    "        steps = [s / scale for s in (8, 16, 32, 64, 100, 300)]\n",
    "        sizes = [s / scale for s in (30, 60, 111, 162, 213, 264, 315)]\n",
    "        aspect_ratios = ((2,), (2,3), (2,3), (2,3), (2,), (2,))\n",
    "        feature_map_sizes = (38, 19, 10, 5, 3, 1)\n",
    "\n",
    "        num_layers = len(feature_map_sizes)\n",
    "\n",
    "        boxes = []\n",
    "\n",
    "\n",
    "        mean = []\n",
    "        x = 0\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            fmsize = feature_map_sizes[i]\n",
    "\n",
    "            for h,w in itertools.product(range(fmsize), repeat=2):\n",
    "                cx = (w+0.5)*steps[i]\n",
    "                cy = (h+0.5)*steps[i]\n",
    "                bbox = [cx,cy, 0.109, 0.12]\n",
    "                boxes.append(bbox)\n",
    "        \n",
    "        self.default_boxes = torch.Tensor(boxes)\n",
    "\n",
    "\n",
    "\n",
    "    def iou(self, box1, box2):\n",
    "        '''Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n",
    "\n",
    "        Args:\n",
    "          box1: (tensor) bounding boxes, sized [N,4].\n",
    "          box2: (tensor) bounding boxes, sized [M,4].\n",
    "\n",
    "        Return:\n",
    "          (tensor) iou, sized [N,M].\n",
    "        '''\n",
    "        N = box1.size(0)\n",
    "        M = box2.size(0)\n",
    "\n",
    "        lt = torch.max(\n",
    "            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "        )\n",
    "\n",
    "        rb = torch.min(\n",
    "            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "        )\n",
    "\n",
    "        wh = rb - lt  # [N,M,2]\n",
    "        wh[wh<0] = 0  # clip at 0\n",
    "        inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n",
    "\n",
    "        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\n",
    "        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\n",
    "        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n",
    "        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n",
    "\n",
    "        iou = inter / (area1 + area2 - inter)\n",
    "        return iou\n",
    "\n",
    "    def encode(self, boxes, classes, threshold=0.5):\n",
    "        '''Transform target bounding boxes and class labels to SSD boxes and classes.\n",
    "\n",
    "        Match each object box to all the default boxes, pick the ones with the\n",
    "        Jaccard-Index > 0.5:\n",
    "            Jaccard(A,B) = AB / (A+B-AB)\n",
    "\n",
    "        Args:\n",
    "          boxes: (tensor) object bounding boxes (xmin,ymin,xmax,ymax) of a image, sized [#obj, 4].\n",
    "          classes: (tensor) object class labels of a image, sized [#obj,].\n",
    "          threshold: (float) Jaccard index threshold\n",
    "\n",
    "        Returns:\n",
    "          boxes: (tensor) bounding boxes, sized [#obj, 8732, 4].\n",
    "          classes: (tensor) class labels, sized [8732,]\n",
    "        '''\n",
    "#         print(f\"Default box shape: {self.default_boxes.shape}\")\n",
    "        default_boxes = self.default_boxes\n",
    "        num_default_boxes = default_boxes.size(0)\n",
    "        num_objs = boxes.size(0)\n",
    "        \n",
    "        \n",
    "        iou = self.iou(  # [#obj,8732]\n",
    "            boxes,\n",
    "            torch.cat([default_boxes[:,:2] - default_boxes[:,2:]/2,\n",
    "                       default_boxes[:,:2] + default_boxes[:,2:]/2], 1)\n",
    "        )\n",
    "\n",
    "#         print(f'iou shape: {iou.shape}')\n",
    "\n",
    "        \n",
    "        iou, max_idx = iou.max(0)  # [1,8732]\n",
    "\n",
    "        max_idx.squeeze_(0)        # [8732,]\n",
    "        iou.squeeze_(0)            # [8732,]\n",
    "        \n",
    "#         print(f'iou sq shape: {iou.shape}')\n",
    "\n",
    "        \n",
    "        boxes = boxes[max_idx]     # [8732,4]\n",
    "        \n",
    "\n",
    "#         print(f'def box shape: {boxes.shape}')\n",
    "         \n",
    "        variances = [0.1, 0.2]\n",
    "        cxcy = (boxes[:,:2] + boxes[:,2:])/2 - default_boxes[:,:2]  # [8732,2]\n",
    "        cxcy /= variances[0] * default_boxes[:,2:]\n",
    "        wh = (boxes[:,2:] - boxes[:,:2]) / default_boxes[:,2:]      # [8732,2]\n",
    "        wh = torch.log(wh) / variances[1]\n",
    "        loc = torch.cat([cxcy, wh], 1)  # [8732,4]\n",
    "\n",
    "        conf = 1 + classes[max_idx]   # [8732,], background class = 0\n",
    "        conf[iou<threshold] = 0       # background\n",
    "        return loc, conf\n",
    "\n",
    "    def nms(self, bboxes, scores, threshold=0.5, mode='union'):\n",
    "        '''Non maximum suppression.\n",
    "\n",
    "        Args:\n",
    "          bboxes: (tensor) bounding boxes, sized [N,4].\n",
    "          scores: (tensor) bbox scores, sized [N,].\n",
    "          threshold: (float) overlap threshold.\n",
    "          mode: (str) 'union' or 'min'.\n",
    "\n",
    "        Returns:\n",
    "          keep: (tensor) selected indices.\n",
    "\n",
    "        Ref:\n",
    "          https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/nms/py_cpu_nms.py\n",
    "        '''\n",
    "        x1 = bboxes[:,0]\n",
    "        y1 = bboxes[:,1]\n",
    "        x2 = bboxes[:,2]\n",
    "        y2 = bboxes[:,3]\n",
    "\n",
    "        areas = (x2-x1) * (y2-y1)\n",
    "        _, order = scores.sort(0, descending=True)\n",
    "\n",
    "        keep = []\n",
    "        while order.numel() > 0:\n",
    "            i = order[0]\n",
    "            keep.append(i)\n",
    "\n",
    "            if order.numel() == 1:\n",
    "                break\n",
    "\n",
    "            xx1 = x1[order[1:]].clamp(min=x1[i])\n",
    "            yy1 = y1[order[1:]].clamp(min=y1[i])\n",
    "            xx2 = x2[order[1:]].clamp(max=x2[i])\n",
    "            yy2 = y2[order[1:]].clamp(max=y2[i])\n",
    "\n",
    "            w = (xx2-xx1).clamp(min=0)\n",
    "            h = (yy2-yy1).clamp(min=0)\n",
    "            inter = w*h\n",
    "\n",
    "            if mode == 'union':\n",
    "                ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "            elif mode == 'min':\n",
    "                ovr = inter / areas[order[1:]].clamp(max=areas[i])\n",
    "            else:\n",
    "                raise TypeError('Unknown nms mode: %s.' % mode)\n",
    "\n",
    "            ids = (ovr<=threshold).nonzero().squeeze()\n",
    "            if ids.numel() == 0:\n",
    "                break\n",
    "            order = order[ids+1]\n",
    "        return torch.LongTensor(keep)\n",
    "\n",
    "    def decode(self, loc, conf):\n",
    "        '''Transform predicted loc/conf back to real bbox locations and class labels.\n",
    "\n",
    "        Args:\n",
    "          loc: (tensor) predicted loc, sized [8732,4].\n",
    "          conf: (tensor) predicted conf, sized [8732,21].\n",
    "\n",
    "        Returns:\n",
    "          boxes: (tensor) bbox locations, sized [#obj, 4].\n",
    "          labels: (tensor) class labels, sized [#obj,1].\n",
    "        '''\n",
    "        variances = [0.1, 0.2]\n",
    "        wh = torch.exp(loc[:,2:]*variances[1]) * self.default_boxes[:,2:]\n",
    "        cxcy = loc[:,:2] * variances[0] * self.default_boxes[:,2:] + self.default_boxes[:,:2]\n",
    "        boxes = torch.cat([cxcy-wh/2, cxcy+wh/2], 1)  # [8732,4]\n",
    "\n",
    "        max_conf, labels = conf.max(1)  # [8732,1]\n",
    "        ids = labels.squeeze(1).nonzero().squeeze(1)  # [#boxes,]\n",
    "\n",
    "        keep = self.nms(boxes[ids], max_conf[ids].squeeze(1))\n",
    "        return boxes[ids][keep], labels[ids][keep], max_conf[ids][keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "peaceful-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(data.Dataset):\n",
    "    img_size = 300\n",
    "\n",
    "    def __init__(self, root=None, df=None, train=None, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "          root: (str) ditectory to images.\n",
    "          list_file: (str) path to index file.\n",
    "          train: (boolean) train or test.\n",
    "          transform: ([transforms]) image transforms.\n",
    "        '''\n",
    "        self.root = root\n",
    "        \n",
    "        \n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        self.fnames = []\n",
    "        self.boxes = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.data_encoder = DataEncoder()\n",
    "        self.df = df\n",
    "        self.bbox_cols = [\"x\", \"y\",\"w\",\"h\"]\n",
    "        self.image_list = os.listdir(root) \n",
    "        self.num_samples = len(self.image_list)\n",
    "        \n",
    "\n",
    "        for img in self.image_list:\n",
    "            \n",
    "            self.fnames.append(img)\n",
    "            split_df = df[df[\"names\"]==img]\n",
    "            cls_ls, bbox = split_df[\"class\"].values, split_df[self.bbox_cols].values      \n",
    "            self.boxes.append(torch.Tensor(bbox))\n",
    "            self.labels.append(torch.LongTensor(cls_ls))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Load a image, and encode its bbox locations and class labels.\n",
    "\n",
    "        Args:\n",
    "          idx: (int) image index.\n",
    "\n",
    "        Returns:\n",
    "          img: (tensor) image tensor.\n",
    "          loc_target: (tensor) location targets, sized [8732,4].\n",
    "          conf_target: (tensor) label targets, sized [8732,].\n",
    "        '''\n",
    "        # Load image and bbox locations.\n",
    "        fname = self.fnames[idx]\n",
    "        img = Image.open(os.path.join(self.root, fname))\n",
    "        boxes = self.boxes[idx].clone()\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # Data augmentation while training.\n",
    "        if self.train:\n",
    "            img, boxes = self.random_flip(img, boxes)\n",
    "            img, boxes, labels = self.random_crop(img, boxes, labels)\n",
    "\n",
    "        # Scale bbox locaitons to [0,1].\n",
    "        w,h = img.size\n",
    "        boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes)\n",
    "\n",
    "        img = img.resize((self.img_size,self.img_size))\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        \n",
    "        # Encode loc & conf targets.\n",
    "        loc_target, conf_target = self.data_encoder.encode(boxes, labels)\n",
    "#         print(f'box shape: {boxes.shape} lable shape: {labels.shape}')\n",
    "\n",
    "        return img, loc_target, conf_target\n",
    "\n",
    "    def random_flip(self, img, boxes):\n",
    "        '''Randomly flip the image and adjust the bbox locations.\n",
    "\n",
    "        For bbox (xmin, ymin, xmax, ymax), the flipped bbox is:\n",
    "        (w-xmax, ymin, w-xmin, ymax).\n",
    "\n",
    "        Args:\n",
    "          img: (PIL.Image) image.\n",
    "          boxes: (tensor) bbox locations, sized [#obj, 4].\n",
    "\n",
    "        Returns:\n",
    "          img: (PIL.Image) randomly flipped image.\n",
    "          boxes: (tensor) randomly flipped bbox locations, sized [#obj, 4].\n",
    "        '''\n",
    "        if random.random() < 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w = img.width\n",
    "            xmin = w - boxes[:,2]\n",
    "            xmax = w - boxes[:,0]\n",
    "            boxes[:,0] = xmin\n",
    "            boxes[:,2] = xmax\n",
    "        return img, boxes\n",
    "\n",
    "    def random_crop(self, img, boxes, labels):\n",
    "        '''Randomly crop the image and adjust the bbox locations.\n",
    "\n",
    "        For more details, see 'Chapter2.2: Data augmentation' of the paper.\n",
    "\n",
    "        Args:\n",
    "          img: (PIL.Image) image.\n",
    "          boxes: (tensor) bbox locations, sized [#obj, 4].\n",
    "          labels: (tensor) bbox labels, sized [#obj,].\n",
    "\n",
    "        Returns:\n",
    "          img: (PIL.Image) cropped image.\n",
    "          selected_boxes: (tensor) selected bbox locations.\n",
    "          labels: (tensor) selected bbox labels.\n",
    "        '''\n",
    "        imw, imh = img.size\n",
    "        while True:\n",
    "            min_iou = random.choice([None, 0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "            if min_iou is None:\n",
    "                return img, boxes, labels\n",
    "\n",
    "            for _ in range(100):\n",
    "                w = random.randrange(int(0.1*imw), imw)\n",
    "                h = random.randrange(int(0.1*imh), imh)\n",
    "\n",
    "                if h > 2*w or w > 2*h:\n",
    "                    continue\n",
    "\n",
    "                x = random.randrange(imw - w)\n",
    "                y = random.randrange(imh - h)\n",
    "                roi = torch.Tensor([[x, y, x+w, y+h]])\n",
    "\n",
    "                center = (boxes[:,:2] + boxes[:,2:]) / 2  # [N,2]\n",
    "                roi2 = roi.expand(len(center), 4)  # [N,4]\n",
    "                mask = (center > roi2[:,:2]) & (center < roi2[:,2:])  # [N,2]\n",
    "                mask = mask[:,0] & mask[:,1]  #[N,]\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                selected_boxes = boxes.index_select(0, mask.nonzero().squeeze(1))\n",
    "\n",
    "                iou = self.data_encoder.iou(selected_boxes, roi)\n",
    "                if iou.min() < min_iou:\n",
    "                    continue\n",
    "\n",
    "                img = img.crop((x, y, x+w, y+h))\n",
    "                selected_boxes[:,0].add_(-x).clamp_(min=0, max=w)\n",
    "                selected_boxes[:,1].add_(-y).clamp_(min=0, max=h)\n",
    "                selected_boxes[:,2].add_(-x).clamp_(min=0, max=w)\n",
    "                selected_boxes[:,3].add_(-y).clamp_(min=0, max=h)\n",
    "                return img, selected_boxes, labels[mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beautiful-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "from ssd import SSD300\n",
    "# from multibox_loss import MultiBoxLoss\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "best_loss = float('inf')  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "trainset = ListDataset(root=\"./dataset/ShelfImages/train/\", df=df, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = ListDataset(root='./dataset/ShelfImages/test/', df = df, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "wired-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    num_classes = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.handle_nan_loss = None\n",
    "\n",
    "    def cross_entropy_loss(self, x, y):\n",
    "        '''Cross entropy loss w/o averaging across all samples.\n",
    "\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "\n",
    "        Return:\n",
    "          (tensor) cross entroy loss, sized [N,].\n",
    "        '''\n",
    "        xmax = x.data.max()\n",
    "        log_sum_exp = torch.log(torch.sum(torch.exp(x-xmax), 1)) + xmax\n",
    "        return log_sum_exp - x.gather(1, y.view(-1,1))\n",
    "\n",
    "    def test_cross_entropy_loss(self):\n",
    "        a = Variable(torch.randn(10,4))\n",
    "        b = Variable(torch.ones(10).long())\n",
    "        loss = self.cross_entropy_loss(a,b)\n",
    "#         print(loss.mean())\n",
    "#         print(F.cross_entropy(a,b))\n",
    "\n",
    "    def hard_negative_mining(self, conf_loss, pos):\n",
    "        '''Return negative indices that is 3x the number as postive indices.\n",
    "\n",
    "        Args:\n",
    "          conf_loss: (tensor) cross entroy loss between conf_preds and conf_targets, sized [N*1940,].\n",
    "          pos: (tensor) positive(matched) box indices, sized [N,1940].\n",
    "\n",
    "        Return:\n",
    "          (tensor) negative indices, sized [N,1940].\n",
    "        '''\n",
    "        batch_size, num_boxes = pos.size()\n",
    "\n",
    "        conf_loss[pos] = 0  # set pos boxes = 0, the rest are neg conf_loss\n",
    "        conf_loss = conf_loss.view(batch_size, -1)  # [N,1940]\n",
    "\n",
    "        _,idx = conf_loss.sort(1, descending=True)  # sort by neg conf_loss\n",
    "        _,rank = idx.sort(1)  # [N,1940]\n",
    "\n",
    "        num_pos = pos.long().sum(1)  # [N,1]\n",
    "        num_neg = torch.clamp(3*num_pos, max=num_boxes-1)  # [N,1]\n",
    "\n",
    "        neg = rank < num_neg.expand_as(rank)  # [N,1940]\n",
    "        return neg\n",
    "\n",
    "    def forward(self, loc_preds, loc_targets, conf_preds, conf_targets):\n",
    "        '''Compute loss between (loc_preds, loc_targets) and (conf_preds, conf_targets).\n",
    "\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [batch_size, 1940, 4].\n",
    "          loc_targets: (tensor) encoded target locations, sized [batch_size, 1940, 4].\n",
    "          conf_preds: (tensor) predicted class confidences, sized [batch_size, 1940, num_classes].\n",
    "          conf_targets: (tensor) encoded target classes, sized [batch_size, 1940].\n",
    "\n",
    "        loss:\n",
    "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + CrossEntropyLoss(conf_preds, conf_targets).\n",
    "        '''\n",
    "        flag = 1\n",
    "        batch_size, num_boxes, _ = loc_preds.size()\n",
    "\n",
    "        pos = conf_targets>0  # [N,1940], pos means the box matched.\n",
    "        num_matched_boxes = pos.data.long().sum()\n",
    "#         print(f'num of matched boxes: {num_matched_boxes}')\n",
    "\n",
    "        if num_matched_boxes == 0:\n",
    "            return self.handle_nan_loss\n",
    "#         else:\n",
    "\n",
    "        ################################################################\n",
    "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
    "        ################################################################\n",
    "        pos_mask = pos.unsqueeze(2).expand_as(loc_preds)    # [N,1940,4]\n",
    "        pos_loc_preds = loc_preds[pos_mask].view(-1,4)      # [#pos,4]\n",
    "        pos_loc_targets = loc_targets[pos_mask].view(-1,4)  # [#pos,4]\n",
    "        loc_loss = F.smooth_l1_loss(pos_loc_preds, pos_loc_targets, size_average=False)\n",
    "        loc_loss/= num_matched_boxes\n",
    "        flag = 1\n",
    "#         print(f'Location loss: {loc_loss}')\n",
    "        self.handle_nan_loss = loc_loss\n",
    "#        \n",
    "        return loc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "oriental-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SSD300()\n",
    "\n",
    "lr = 1e-3\n",
    "use_cuda = False\n",
    "\n",
    "criterion = MultiBoxLoss()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), \n",
    "                      lr=lr,\n",
    "                      momentum=0.9,\n",
    "                      weight_decay=1e-4\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adverse-science",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 38, 38])\n",
      "9.275 9.275\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.616 10.446\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.616 10.836\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.616 11.031\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.536 10.932\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.536 10.866\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.119 10.759\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.916 10.654\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.719 10.550\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.985 10.694\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.454 10.581\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.557 10.496\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.474 10.417\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.834 10.375\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.834 10.339\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.896 10.312\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.068 10.356\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.000 10.281\n",
      "torch.Size([4, 512, 38, 38])\n",
      "8.582 10.191\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.595 10.162\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.914 10.197\n",
      "torch.Size([4, 512, 38, 38])\n",
      "12.149 10.286\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.699 10.304\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.699 10.321\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.561 10.330\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.982 10.394\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.276 10.426\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.316 10.458\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.781 10.435\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.353 10.432\n",
      "torch.Size([4, 512, 38, 38])\n",
      "7.941 10.352\n",
      "torch.Size([4, 512, 38, 38])\n",
      "7.941 10.276\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.352 10.279\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.384 10.282\n",
      "torch.Size([4, 512, 38, 38])\n",
      "8.202 10.222\n",
      "torch.Size([4, 512, 38, 38])\n",
      "8.202 10.166\n",
      "torch.Size([4, 512, 38, 38])\n",
      "8.922 10.133\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.809 10.150\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.214 10.178\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.461 10.210\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.461 10.240\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.461 10.269\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.999 10.286\n",
      "torch.Size([4, 512, 38, 38])\n",
      "7.322 10.219\n",
      "torch.Size([4, 512, 38, 38])\n",
      "7.322 10.155\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.780 10.146\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.836 10.140\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.928 10.156\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.702 10.147\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.578 10.156\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.425 10.141\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.425 10.127\n",
      "torch.Size([4, 512, 38, 38])\n",
      "12.846 10.179\n",
      "torch.Size([4, 512, 38, 38])\n",
      "12.846 10.228\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.316 10.230\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.853 10.223\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.750 10.215\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.562 10.221\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.562 10.227\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.484 10.231\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.882 10.241\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.882 10.252\n",
      "torch.Size([4, 512, 38, 38])\n",
      "10.687 10.259\n",
      "torch.Size([4, 512, 38, 38])\n",
      "12.175 10.289\n",
      "torch.Size([4, 512, 38, 38])\n",
      "12.175 10.318\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.437 10.304\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.437 10.291\n",
      "torch.Size([4, 512, 38, 38])\n",
      "12.813 10.328\n",
      "torch.Size([4, 512, 38, 38])\n",
      "9.364 10.315\n",
      "torch.Size([4, 512, 38, 38])\n",
      "11.282 10.328\n",
      "torch.Size([3, 512, 38, 38])\n",
      "12.331 10.357\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (images, loc_targets, conf_targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            loc_targets = loc_targets.cuda()\n",
    "            conf_targets = conf_targets.cuda()\n",
    "\n",
    "\n",
    "        images = Variable(images)\n",
    "\n",
    "        loc_targets = Variable(loc_targets)\n",
    "\n",
    "        conf_targets = Variable(conf_targets)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loc_preds, conf_preds = net(images)\n",
    "\n",
    "    #     try:\n",
    "\n",
    "        loss = Variable(criterion(loc_preds, loc_targets, \n",
    "                                      conf_preds, conf_targets),requires_grad =True)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data\n",
    "        print('%.3f %.3f' % (loss.data, train_loss/(batch_idx+1)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (images, loc_targets, conf_targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            loc_targets = loc_targets.cuda()\n",
    "            conf_targets = conf_targets.cuda()\n",
    "\n",
    "        images = Variable(images)\n",
    "        loc_targets = Variable(loc_targets)\n",
    "        conf_targets = Variable(conf_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loc_preds, conf_preds = net(images)\n",
    "        loss = criterion(loc_preds, loc_targets, conf_preds, conf_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        print('%.3f %.3f' % (loss.data[0], train_loss/(batch_idx+1)))\n",
    "\n",
    "def test(epoch):\n",
    "    print('\\nTest')\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    for batch_idx, (images, loc_targets, conf_targets) in enumerate(testloader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            loc_targets = loc_targets.cuda()\n",
    "            conf_targets = conf_targets.cuda()\n",
    "\n",
    "        images = Variable(images, volatile=True)\n",
    "        loc_targets = Variable(loc_targets)\n",
    "        conf_targets = Variable(conf_targets)\n",
    "\n",
    "        loc_preds, conf_preds = net(images)\n",
    "        loss = criterion(loc_preds, loc_targets, conf_preds, conf_targets)\n",
    "        test_loss += loss.data[0]\n",
    "        print('%.3f %.3f' % (loss.data[0], test_loss/(batch_idx+1)))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    global best_loss\n",
    "    test_loss /= len(testloader)\n",
    "    if test_loss < best_loss:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.module.state_dict(),\n",
    "            'loss': test_loss,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_loss = test_loss\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-image",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-gothic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-metabolism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
